[2024-02-21 21:35:33,488][flwr][WARNING] - 
Setting `min_available_clients` lower than `min_fit_clients` or
`min_evaluate_clients` can cause the server to fail when there are too few clients
connected to the server. `min_available_clients` must be set to a value larger
than or equal to the values of `min_fit_clients` and `min_evaluate_clients`.

[2024-02-21 21:35:33,499][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2024-02-21 21:35:37,040][flwr][INFO] - Flower VCE: Ray initialized with resources: {'CPU': 8.0, 'node:127.0.0.1': 1.0, 'object_store_memory': 2147483648.0, 'node:__internal_head__': 1.0, 'memory': 7069769728.0}
[2024-02-21 21:35:37,041][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
[2024-02-21 21:35:37,041][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 4, 'num_gpus': 0}
[2024-02-21 21:35:37,056][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
[2024-02-21 21:35:37,056][flwr][INFO] - Initializing global parameters
[2024-02-21 21:35:37,057][flwr][INFO] - Using initial parameters provided by strategy
[2024-02-21 21:35:37,057][flwr][INFO] - Evaluating initial parameters
[2024-02-21 21:35:37,080][flwr][ERROR] - Error(s) in loading state_dict for SimpleNet:
	size mismatch for conv1.weight: copying a param with shape torch.Size([16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([6, 3, 5, 5]).
	size mismatch for conv1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([6]).
	size mismatch for conv2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([16, 6, 5, 5]).
	size mismatch for fc1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([120, 400]).
	size mismatch for fc1.bias: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([120]).
	size mismatch for fc2.weight: copying a param with shape torch.Size([16, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([84, 120]).
	size mismatch for fc2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([84]).
	size mismatch for fc3.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([10, 84]).
	size mismatch for fc3.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([10]).
[2024-02-21 21:35:37,085][flwr][ERROR] - Traceback (most recent call last):
  File "/Users/Slaton/myvenv/flwr/lib/python3.9/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/Users/Slaton/myvenv/flwr/lib/python3.9/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/Users/Slaton/myvenv/flwr/lib/python3.9/site-packages/flwr/server/server.py", line 92, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
  File "/Users/Slaton/myvenv/flwr/lib/python3.9/site-packages/flwr/server/strategy/fedavg.py", line 165, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/Users/Slaton/Documents/grenoble-code/fl-flower/project3-result/server.py", line 19, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/Users/Slaton/myvenv/flwr/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2153, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for SimpleNet:
	size mismatch for conv1.weight: copying a param with shape torch.Size([16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([6, 3, 5, 5]).
	size mismatch for conv1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([6]).
	size mismatch for conv2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([16, 6, 5, 5]).
	size mismatch for fc1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([120, 400]).
	size mismatch for fc1.bias: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([120]).
	size mismatch for fc2.weight: copying a param with shape torch.Size([16, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([84, 120]).
	size mismatch for fc2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([84]).
	size mismatch for fc3.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([10, 84]).
	size mismatch for fc3.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([10]).

[2024-02-21 21:35:37,086][flwr][ERROR] - Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 4, 'num_gpus': 0} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 4, 'num_gpus': 0}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
