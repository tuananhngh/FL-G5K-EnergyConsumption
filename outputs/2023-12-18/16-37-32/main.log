[2023-12-18 16:37:34,443][flwr][WARNING] - 
Setting `min_available_clients` lower than `min_fit_clients` or
`min_evaluate_clients` can cause the server to fail when there are too few clients
connected to the server. `min_available_clients` must be set to a value larger
than or equal to the values of `min_fit_clients` and `min_evaluate_clients`.

[2023-12-18 16:37:34,444][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=50, round_timeout=None)
[2023-12-18 16:37:38,178][flwr][INFO] - Flower VCE: Ray initialized with resources: {'memory': 122216065229.0, 'node:172.16.36.6': 1.0, 'object_store_memory': 56664027955.0, 'accelerator_type:P100': 1.0, 'CPU': 48.0, 'GPU': 2.0, 'node:__internal_head__': 1.0}
[2023-12-18 16:37:38,178][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
[2023-12-18 16:37:38,179][flwr][INFO] - No `client_resources` specified. Using minimal resources for clients.
[2023-12-18 16:37:38,179][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}
[2023-12-18 16:37:38,210][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 48 actors
[2023-12-18 16:37:38,211][flwr][INFO] - Initializing global parameters
[2023-12-18 16:37:38,211][flwr][INFO] - Requesting initial parameters from one random client
[2023-12-18 16:37:43,282][flwr][INFO] - Received initial parameters from one random client
[2023-12-18 16:37:43,282][flwr][INFO] - Evaluating initial parameters
[2023-12-18 16:37:44,603][flwr][ERROR] - Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor
[2023-12-18 16:37:44,607][flwr][ERROR] - Traceback (most recent call last):
  File "/home/tunguyen/.conda/envs/flower-simulation/lib/python3.11/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
           ^^^^^^^
  File "/home/tunguyen/.conda/envs/flower-simulation/lib/python3.11/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tunguyen/.conda/envs/flower-simulation/lib/python3.11/site-packages/flwr/server/server.py", line 92, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tunguyen/.conda/envs/flower-simulation/lib/python3.11/site-packages/flwr/server/strategy/fedavg.py", line 163, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tunguyen/federated-learning/project3/server.py", line 21, in evaluate_fn
    loss, accuracy = test(model, testloader, device, verbose=False) # testloader is global
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tunguyen/federated-learning/project3/model.py", line 69, in test
    outputs = model(images)
              ^^^^^^^^^^^^^
  File "/home/tunguyen/.conda/envs/flower-simulation/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tunguyen/.conda/envs/flower-simulation/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tunguyen/federated-learning/project3/model.py", line 24, in forward
    x = self.pool(F.relu(self.conv1(x)))
                         ^^^^^^^^^^^^^
  File "/home/tunguyen/.conda/envs/flower-simulation/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tunguyen/.conda/envs/flower-simulation/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tunguyen/.conda/envs/flower-simulation/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tunguyen/.conda/envs/flower-simulation/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor

[2023-12-18 16:37:44,607][flwr][ERROR] - Your simulation crashed :(. This could be because of several reasons.The most common are: 
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 1, 'num_gpus': 0.0} is not enough for your workload). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 1, 'num_gpus': 0.0}.
